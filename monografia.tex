% Numeração de acordo com UFPR
\documentclass[pnumromarab,normaltoc]{abnt}	

\usepackage[brazil]{babel}
\usepackage[latin1]{inputenc}
\usepackage{abnt-alf}
\usepackage{graphicx}
\usepackage{url}

\makeatletter	% Para que ele entenda o @

% CAPA
%\renewcommand{\capa} {
%\begin{titlepage}
%	\espaco{1.1}
%	
%	\begin{center}
%		\large\ABNTchapterfont\ABNTautordata
%	\end{center}
%	
%	\vspace{7.5cm}
%	
%	\begin{center}
%		\large\ABNTchapterfont\ABNTtitulodata\par
%	\end{center}
%	
%	\vfill
%	
%	\begin{center}
%		\textbf{\ABNTlocaldata}\par
%		\textbf{\ABNTdatadata}
%	\end{center}
%\end{titlepage}
%}

% FOLHA DE ROSTO
\newcommand{\esporient}[2] {
	\leftskip 0em
	\@tempdima 5.5em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{#1#2\hfil}
}

\newcommand{\espcoorient}[2] {
	\leftskip 0em
	\@tempdima 7em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{#1#2\hfil}
}

\renewcommand{\folhaderosto} {
\begin{titlepage}
	\espaco{1.1}
	
	\begin{center}
		\large\ABNTchapterfont\ABNTautordata
	\end{center}
	
	\vspace{7.5cm}
	
	\begin{center}
		\large\ABNTchapterfont\ABNTtitulodata\par
	\end{center}
	
	\vspace{2cm}
	
	\hspace{.35\textwidth}
	\begin{minipage}{.5\textwidth}
		\begin{espacosimples}
			\ABNTcomentariodata\par
		\end{espacosimples}
	\end{minipage}
	
	\hspace{.35\textwidth}
	\begin{minipage}{.5\textwidth}
		\begin{espacosimples}
			\esporient{\numberline {Orientador:}}{\ignorespaces\ABNTorientadordata}
		\end{espacosimples}
	\end{minipage}
	
	\ABNTifnotempty{\ABNTcoorientadordata}{
		\hspace{.35\textwidth}
		\begin{minipage}{.5\textwidth}
			\begin{espacosimples}
				\espcoorient{\numberline {Co-Orientador:}}{\ignorespaces\ABNTcoorientadordata}
			\end{espacosimples}
		\end{minipage}}
	
	\vfill
	
	\begin{center}
		\textbf{\ABNTlocaldata}\par
		\textbf{\ABNTdatadata}
	\end{center}

\end{titlepage}
}

% Altera o tamanho das fontes dos capítulos e dos apêndices
\renewcommand{\ABNTchapterfont}{\bfseries}
\renewcommand{\ABNTchaptersize}{\Large}
\renewcommand{\ABNTanapsize}{\Large}

% Altera o espaçamento entre dots
\renewcommand\@dotsep{2}

% Altera forma de montagem do TOC
\renewcommand\l@chapter[2]{
  \ifnum \c@tocdepth >\m@ne
    \addpenalty{-\@highpenalty}%
    \vskip 1.0em \@plus\p@
    \setlength\@tempdima{1.5em}%
    \begingroup
      \ifthenelse{\boolean{ABNTpagenumstyle}}
        {\renewcommand{\@pnumwidth}{3.5em}}
        {}
      \parindent \z@ \rightskip \@pnumwidth
      \parfillskip -\@pnumwidth
      \leavevmode \normalsize\ABNTtocchapterfont
      \advance\leftskip\@tempdima
      \hskip -\leftskip
      #1\nobreak\dotfill \nobreak%
      \ifthenelse{\boolean{ABNTpagenumstyle}}
         {%
          \hb@xt@\@pnumwidth{\hss 
            \ifthenelse{\not\equal{#2}{}}{{\normalfont p.\thinspace#2}}{}}\par
         }
         {%
          \hb@xt@\@pnumwidth{\hss #2}\par
         }
      \penalty\@highpenalty
    \endgroup
  \fi
}

\renewcommand*\l@section{\@dottedtocline{1}{0em}{2.3em}}
\renewcommand*\l@subsection{\@dottedtocline{2}{0em}{3.2em}}
\renewcommand*\l@subsubsection{\@dottedtocline{3}{0em}{4.1em}}

% Cria um comando auxiliar para montagem da lista de figuras
%\newcommand{\figfillnum}[1]{%
%  {\hspace{1em}\normalfont\dotfill}\nobreak
%  \hb@xt@\@pnumwidth{\hfil\normalfont #1}{}\par}

% Cria um comando auxiliar para montagem da lista de tabelas
%\newcommand{\tabfillnum}[1]{%
%	{\hspace{1em}\normalfont\dotfill}\nobreak
%	\hb@xt@\@pnumwidth{\hfil\normalfont #1}{}\par}

% Altera a forma de montagem da lista de figuras
\renewcommand*{\l@figure}[2]{
	\leftskip 3.1em
	\rightskip 1.6em
	\parfillskip -\rightskip
	\parindent 0em
	\@tempdima 2.0em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{Figura \normalfont #1}\nobreak \figfillnum{#2}}

% Altera a forma de montagem de lista de tabelas
\renewcommand*{\l@table}[2]{
	\leftskip 3.4em
	\rightskip 1.6em
	\parfillskip -\rightskip
	\parindent 0em
	\@tempdima 2.0em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{Tabela \normalfont #1}\nobreak \tabfillnum{#2}}

% Define os comandos que montam a lista de símbolos
\newcommand{\listadesimbolos}{\pretextualchapter{Lista de Símbolos}\@starttoc{lsb}}
\newcommand{\simbolo}[2]{{\addcontentsline{lsb}{simbolo}{\numberline{#1}{#2}}}#1}
\newcommand{\l@simbolo}[2]{
	\vspace{-0.75cm}
	\leftskip 0em
	\parindent 0em
	\@tempdima 5em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{\normalfont #1}\hfil\nobreak\par}

% Define o comando que monta a lista de siglas
\newcommand{\listadesiglas}{\pretextualchapter{Lista de Siglas}\@starttoc{lsg}}
\newcommand{\sigla}[2]{{\addcontentsline{lsg}{sigla}{\numberline{#1}{#2}}}#1}
\newcommand{\l@sigla}[2]{
	\vspace{-0.75cm}
	\leftskip 0em
	\parindent 0em
	\@tempdima 5em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{\normalfont #1}\hfil\nobreak\par}

% Define o tipo de numeração das páginas
\renewcommand{\chaptertitlepagestyle}{plain}

% Altera a posição da numeração de páginas dos elementos pré-textuais
\renewcommand\pretextualchapter{
	\if@openright\cleardoublepage\else\clearpage\fi
	\pagestyle{\chaptertitlepagestyle}
	\global\@topnum\z@
	\@afterindentfalse
	\@schapter}

% Altera a posição da numeração de páginas dos elementos textuais
\renewcommand{\ABNTchaptermark}[1]{
	\ifthenelse{\boolean{ABNTNextOutOfTOC}}
		{\markboth{\ABNTnextmark}{\ABNTnextmark}}
		{\chaptermark{#1}
		\pagestyle{\chaptertitlepagestyle}}}

% Redefine o tipo de numeração das páginas
\renewcommand{\ABNTBeginOfTextualPart}{
	\renewcommand{\chaptertitlepagestyle}{plainheader}
	\renewcommand{\thepage}{\arabic{page}}
	\setcounter{page}{1}}

\makeatother

% Altera o tamanho do parágrafo
\setlength{\parindent}{1.5cm}

% DOCUMENTO
\begin{document}

\autor{CAIO ESDRAS DE BRITO BEGOTTI}
\titulo{MÉTODO PARA CRIAÇÃO E ANÁLISE DE CORPORA LINGUÍSTICOS EM LATIM CLÁSSICO UTILIZANDO NLTK}
\orientador{Alessandro Rolim de Moura}

% Se tiver co-orientador
%\coorientador{Co-orientador 1\protect\\Co-orientador 2}

\comentario{Projeto de Monografia apresentado à disciplina Orientação Monográfica I do Curso de Letras (Bacharelado em Estudos Linguísticos), do Setor de Ciências Humanas, Letras e Artes da Universidade Federal do Paraná.}

\local{CURITIBA}

\data{DEZEMBRO DE 2011}

%\capa
\folhaderosto

% TERMO DE APROVAÇÃO
%\begin{titlepage}
%	\espaco{1.1}
%	
%	\begin{center}
%		\Large\textbf{{Termo de Aprovação}}
%	\end{center}
%	
%	\vspace{0.75cm}
%	
%	\begin{center}
%		\large\ABNTautordata
%	\end{center}
%	
%	\vspace{1cm}
%	
%	\begin{center}
%		\large\ABNTtitulodata
%	\end{center}
%	
%	\vspace{1cm}
%	
%	\noindent Dissertação aprovada como requisito parcial para obtenção do grau de Mestre/Doutor em Minha Área, pelo Programa de Pós-Graduação, Setor, Universidade Federal do Paraná, pela seguinte banca examinadora:
%	
%	\setlength{\ABNTsignthickness}{0.4pt}
%	\setlength{\ABNTsignskip}{2cm}
%	
%	\vspace{-0.5cm}
%	\assinatura{Prof. Dr. Meu Orientador\\Universidade Federal do Paraná}
%	
%	\vspace{-0.5cm}
%	\assinatura{Prof. Dr. Meu Co-orientador\\Universidade Federal do Paraná}
%	
%	\vspace{-0.5cm}
%	\assinatura{Prof. Dr. Convidado\\Universidade} 
%	
%	\vspace{-0.5cm}
%	\assinatura{Prof. Dr. Convidado \\Universidade}
%	
%	\vfill
%	
%	\begin{center}
%		Curitiba, XX de ZZ de YYYY
%	\end{center}
%
%\end{titlepage}

% Deve ser adicionado ao contador de páginas um, referente a folha de rosto.
% A folha de aprovação não recebe número, nem é contada.
\addtocounter{page}{1}

%\pretextualchapter{Dedicatória}
%
%\vspace{12cm}
%\hspace{.3\textwidth}
%\begin{minipage}{.6\textwidth}
%	\par A quem eu dedico,
%	\par $\phantom{linha em branco}$
%	\par Por muito que fizeram, eu fiz tudo, só agradeço por educação...
%\end{minipage}

\newpage

%\pretextualchapter{Agradecimentos}
%
%\vspace{12cm}
%\hspace{.3\textwidth}
%\begin{minipage}{.6\textwidth}
%	\par A todos que, direta ou indiretamente, contribuíram para a realização e divulgação deste trabalho.
%\end{minipage}

%\pretextualchapter{Epígrafe}
%
%\vspace*{12cm}
%\hspace{.3\textwidth}
%\begin{minipage}{.6\textwidth}
%	\par Uma pequena prosa que esteja relacionada com o trabalho.
%	\par Obs.: A forma como uma prosa é escrita é importante.
%\end{minipage}

\sumario

% 1 - Lista de Figuras
%\listadefiguras

% 2 - Lista de Tabelas
%\listadetabelas

% 3 - Lista de Siglas, e.g. \sigla{sigla}{Descrição}
\listadesiglas

% 4 - Lista de Símbolos, e.g. \simbolo{símbolo}{Descrição}
%\listadesimbolos

% RESUMO
%\begin{resumo}
%	$\phantom{linha em branco}$\\
%	Escreva aqui o texto de seu resumo...\\
%	$\phantom{linha em branco}$\\
%	Palavras-chave: Escreva; Aqui; Suas; Palavras-chave.
%\end{resumo}

% ABSTRACT
%\begin{abstract}
%	$\phantom{linha em branco}$\\
%	Write here the English version of your `Resumo'...\\
%	$\phantom{linha em branco}$\\
%	Key-words: Write; Here; Your; Key-words.
%\end{abstract}

% A parte textual deve conter:
% 1 - Introdução
% 2 - Revisão de Literatura
% 3 - Material e Métodos
% 4 - Análise dos Resultados
% 5 - Discussão
% 6 - Conclusão

\chapter{Introdução}
\par O projeto de monografia que aqui se apresenta foca na construção de corpora linguísticos de textos do período clássico do latim e em uma análise computacional desses corpora. O propósito por trás dessas duas tarefas é o de sugerir melhoras em métodos de ensino do latim e permitir uma melhor compreensão dessa língua de forma mais empírica, ou seja, uma compreensão baseada no uso que um dia foi dado a língua através da análise de textos reais em latim clássico.

\par Para tal, esse trabalho se baseará no projeto \sigla{NLTK}{Natural Language Toolkit} (Natural Language Toolkit), que consiste em um conjunto de rotinas de programação de computador, em forma de kit de ferramentas\footnote{A descrição pode soar estranha em português e para não iniciados em programação; em bibliografias de computação ambos os termos aparecem simplesmente como \sigla{API}{Application Programming Interface} e toolkit, em inglês}, para processamento de linguagem natural. Com a ajuda do NLTK e à luz da Linguística de Corpus (\sigla{LC}{Linguística de Corpus}) procurarei evidenciar os usos mais comuns do latim clássico e irei sugerir uma abordagem para uso disso em sala de aula.

\par Marcus Tullius Cicero, ou simplesmente Cícero, será o autor escolhido para a montagem dos corpora. Cícero é um dos autores que mais representam o período clássico do latim --- tanto pela grande produção e variedade de estilos de seus textos quanto pelos seus discursos ---, e devido a sua importância após a Renascença nada mais natural do que usá-lo nas análises desse trabalho.

\par Espera-se ainda, com esse trabalho, contribuir para o ainda pequeno grupo de linguistas computacionais no Brasil, especialmente os que apreciam e trabalham com o latim. A exemplo do que já foi feito com o inglês utilizando métodos semelhantes ao que aqui serão apresentados, pode-se ganhar bastante em sala de aula e pesquisa com tal abordagem: aproximando a linguística de corpus à linguística computacional e línguas naturais \cite{sardinha-palavras}.

\chapter{Justificativas}
\par A seguir se apresentam os problemas e as propostas que justificam este trabalho.

\section{Problemas a serem solucionados}
\par É ponto pacífico que atualmente o ensino de latim se dá de forma bastante limitada em escolas e universidades brasileiras. Todavia, o latim clássico parece estar em uma espécie de retomada em países como Inglaterra (que  vale dizer sempre foi um centro de referência em estudos clássicos) e Estados Unidos. Nos Estados Unidos, por exemplo, relatórios mostram o latim entre as oito línguas mais estudadas hoje no país, na frente do russo e português com crescimento de mais de 20\% em matrículas nos últimos anos \cite{mla}. De qualquer maneira, parte dessas matrículas ainda é para cursos introdutórios onde alunos pouco conhecem sobre a língua e dependem exclusivamente dos professores.

\par Muitas vezes o latim acaba sendo ensinado somente de cima para baixo, ou seja, dos professores aos alunos, pois os primeiros são uma fonte natural de conhecimento e autoridade. Assim, alunos de latim com frequência simplesmente tem que confiar na capacidade de julgamento dos mestres na escolha do melhor material a ser estudado. O que de fato faz sentido, pois os professores sabem avaliar a melhor linha de aprendizado. Mas embora os professores possam fazer boas escolhas quanto ao textos e ao vocabulário a serem estudados, tais escolhas serão sempre parciais ou limitadas, de modo que a criação de materiais e métodos que fundamentem tais escolhas de forma objetiva é sempre bem-vinda.

\par Por si só isso não representa um grande problema. Entretanto, por se tratar de uma língua sem usuários nativos --- logo, poucos podem julgar se o que está sendo estudado de fato representaria a realidade dos usuários de uma língua de dois mil anos atrás --- ninguém na realidade sabe dizer com total certeza se, por exemplo, a seleção de textos e a escolha de determinado vocabulário de estudo são ou não adequados, ou ainda se ele é suficientemente representativo para os alunos ganharem proficiência mais rapidamente.

\par Isso é até aceitável em línguas estrangeiras modernas, afinal o aluno tem mais contato com elas no seu dia-a-dia e pode sair de um beco sem saída com mais facilidade e sem recorrer à ajuda dos professores, mas com línguas clássicas como o latim, e o grego, o prejuízo pode se tornar maior para o aluno que se dedicou por tanto tempo e então percebe que tomou um caminho de estudo não muito bom e perdeu uma melhor oportunidade de aprendizado. É claro que espera-se dos alunos que eles possam compensar eventuais lacunas desse aprendizado através de estudo e leituras próprias, mas infelizmente não se pode contar com isso sempre.

\par Em uma situação limite, um estudante de latim clássico poderia estar aprendendo um vocabulário que outrora pertenceu a um texto menor e que não corresponde a realidade linguística que ele buscava, ou que também não facilita a assimilação de idéias do estilo que procurava entender. Naturalmente, deve-se considerar aqui que o ensino de latim costumava focar aspectos altamente literários e clássicos. Por exemplo, alunos de latim comumente lêem Cícero após um ou dois anos de estudo, mas Cícero somente representa uma fatia pequena do todo que foi o latim em sua época. Deixa-se então o latim vulgar, falado pelo povo, para aulas de linguística românica e textos religiosos para aulas sobre latim medieval ou eclesiástico.

\section{Diferenciais tecnológicos}
\par Parece que um dos motivos pelo qual isso ocorre é a falta de um catálogo linguístico cientificamente montado com as palavras de fato mais usadas da língua. Já houve tentativas de se fazer isso para o latim, porém em uma época cujos poderes da análise linguística feita pelo computador ainda não eram conhecidos \cite{diederich}, ou foram tentativas focadas em outras línguas, como o inglês \cite{almeida-dicasl} e o espanhol \cite{jacobi}. Outros autores, como \citeonline{pellegrino}, no máximo se limitaram a criar listas de palavras bastante usadas por Cícero a serem estudadas para exames de latim de escolas americanas, sem uma maior racionalização sobre os corpora de Cícero e sem também elaborar em seus métodos. Um trabalho bastante superficial e parecendo-se mais como manual de prova. Embora os resultados de todas essas experiências tenham sido utilizados de maneira distinta ao que aqui se propõe, a motivação é similar: utilizar artefatos reais da língua para o aprendizado dela mesma.

\par Criando-se hoje corpora linguísticos do latim em um formato reconhecido por linguistas computacionais, utilizando-se métodos documentados e comprovadamente eficientes para análise linguística, é possível melhorarmos o ensino de latim em escolas e universidades, além de permitir ao aluno --- independente ou aconselhado --- que tome o ensino da língua nas suas próprias mãos. Como bem diz \citeonline{jacobi}, \textit{essa forma de trabalho enfatiza o desenvolvimento da habilidade de descoberta nos alunos. A aprendizagem movida a dados posiciona o aluno no papel de descobridor ou de pesquisador, e o professor passa a ter como função primordial propiciar meios para que os alunos desenvolvam estratégias de descoberta}. Para tal, o volume de dados a ser analisado e utilizado nem mesmo precisa ser muito grande para se notar bons ganhos no aprendizado \cite{willis}.

\par Ainda que existam diversos modelos, bases de dados e programas de computador para a análise de corpora, muitos possuem seus códigos fechados ou requerem licenciamentos, ou seja, não permitem o uso irrestrito deles por parte dos usuários e pesquisadores caso estes desejem, por exemplo, aplicar melhorias nesses softwares ou corrigir falhas nas análises por conta própria. Além disso, alguns são bastante caros --- o que porém é compreensível dada a dificuldade de catalogar e organizar bases enormes ---, e fogem da realidade acadêmica brasileira, ainda carente de recursos para latinistas\footnote{Softwares para análise linguística, por mais simples que sejam suas funcionalidades, não custam menos que algumas dezenas de dólares, quando não centenas. Bancos de dados prontos com corpora costumam também cobrar licenças de uso temporário que podem em alguns casos chegar a milhares de euros ou, embora ``gratuitas'', acabam restringindo o uso e pesquisa comercial através deles.}. Logo, uma solução aberta, sem nenhum tipo de restrição e preferivelmente barata se faz necessária. Essa solução, que será detalhada nas próximas seções, deve ainda aproveitar a possibilidade de utilizar textos sem amarras de copyright como no caso de textos clássicos em latim. Portanto, não está sendo proposta aqui uma revolução no jeito de trabalhar o latim com computação, mas simplesmente uma alternativa viável, barata e eficiente para alunos, pesquisadores e entusiastas brasileiros.

\section{Melhorias no ensino}
\par A busca pelo domínio da leitura em uma segunda língua é um dos maiores objetivos de qualquer interessado em línguas, e isso não é diferente para o latim. Esse trabalho se propõe a ajudar nisso. A motivação inicial então veio através de um projeto similar focado na língua inglesa na UNICAMP. Hoje transformado em livro e guia de estudo, o projeto começou como um mero catálogo das palavras mais comuns do inglês, e desencadeou novas formas de estudar a língua que poderiam beneficiar também o latim \cite{almeida-livro}. De forma análoga, esperamos que alunos de latim consigam se beneficiar do mesmo modelo de aprendizado, comprovado para o inglês. Dessa maneira, poderemos encontrar respostas, que não são facilmente comprováveis sem métodos como os propostos nesse trabalho, para perguntas como: \begin{enumerate} \item que vocabulário é mais adequado para determinados alunos ou pesquisadores em um contato inicial com o latim? \item dos estilos de textos clássicos em latim deixados por Cícero, qual se sobressaía mais? \item que tipo de palavreado era mais comum em textos oficiais, e qual era mais comum em textos informais (como em cartas)? \item quais seriam as palavras mais comuns do latim clássico, muito possivelmente sendo as mais úteis para um início mais veloz do aprendizado?\end{enumerate}

A força que instiga esse trabalho a responder essas questões é muito bem explicada por \citeonline{sapir}, que já acreditava que \textit{vocabulary is a very sensitive index of the culture of a people and changes of meaning, loss of old words, the creation and borrowing of new ones are all dependent on the history of culture itself. Languages differ widely in the nature of their vocabularies. Distinctions which seem inevitable to us may be utterly ignored in languages which reflect an entirely different type of culture, while these in turn insist on distinctions which are all but unintelligible to us. Such differences of vocabulary go far beyond the names of cultural objects, such as arrow point, coat of armor or gunboat. They apply just as well to the mental world}.

\par Enfim, tal abordagem se mostra além de eficiente, bastante realista, ao passo que aponta para os alunos somente o que vale a pena ser aprendido e aprendido não através de contextos artificiais ou idealizados da  língua.

\chapter{Fundamentação Teórica}

\section{Linguística Aplicada e Computacional}
\par Todo esse trabalho se sustenta sobre os princípios da linguística aplicada, ou seja, o uso de ferramentas linguísticas destinado à resolução ou análise de problemas reais das línguas, ou seja, é o estudo prático e não teórico delas. Particularmente, o subcampo da linguística aplicada mais em voga atualmente é o da linguística computacional, que de forma simplificada é um campo de estudo que envolve diversas áreas, como ciência da computação, matemática, neurologia e a própria linguística, para citar os mais populares.

\par A história da linguística computacional está, naturalmente, totalmente ligada ao surgimento dos primeiros computadores, logo após a segunda guerra mundial. Muitos problemas linguísticos hoje envolvem resolver algorítimos ou modelar dados, com frequência em grande escala, e foi exatamente para estes tipos de tarefas que os computadores foram criados. Logo, não é surpresa alguma que ambas as áreas estejam hoje tão próximas (no mundo privado ao menos, pois na academia brasileira tenho a impressão que ainda não chegamos lá) \cite{ladeira}.

\par Do contato da linguística e da computação, então, chegamos ao \sigla{PLN}{Processamento de Linguagem Natural} (Processamento de Linguagem Natural), ou como NLP de \sigla{NLP}{Natural Language Processing} (Natural Language Processing) ?, que basicamente foca em problemas como análise automática de discurso, tradução por máquinas, análises morfo-sintáticas, reconhecimento ou geração de fala, segmentação de anunciados, stemming (redução de palavras até se encontrar raízes comuns), gramática categorial, análises estatísticas, entre diversos outros. A cada dia surgem novas aplicações possíveis, que acabam estreitando a diferença de significado entre linguística computacional e processamento de linguagem natural, que para alguns se distinguem somente como uma sendo o campo teórico e outra o prático da língua dentro da computação.

\section{Linguística de Corpus}
\par Dentro da linguística computacional encontramos afinal a linguística de corpus, campo da linguística que estuda a língua como objeto vivo através de modelos estatísticos e amostragens de dados reais da língua, não representações idealizadas dela. 

\par Para os aderentes à essa linha teórica, a língua deve se expressar por si mesma, pelos textos e discursos que surgem naturalmente de seus usuários, e daí o linguista pode aferir fatos e analisá-la. A linguística de corpus, dessa maneira, evita trabalhar com textos criados artificialmente, e seus modelos são modelos de dados e não modelos teóricos de representação da língua; podem ser textos escritos ou falas transcritas, organizados em um dado formato e com categorização padronizada, além de poderem ser manualmente mapeados para um processamento posterior.

\par Antigamente a linguística de corpus se encontrava em um estado bastante primitivo, muitas vezes sendo trabalhada por formas manuais. Hoje em dia ela é primariamente um campo de estudo que anda junto com a linguística computacional, dado o poder de processamento e automatização de um computador a serviço do linguista \cite{sinclair}, comparado a um linguista solitário com papel e lápis.

\par Somente nas últimas décadas a linguística de corpus tomou o palco das ciências das línguas (por exemplo, foi somente nos anos 70 que o primeiro corpus de língua falada foi montado e analisado digitalmente). Portanto, não é de surpreender que tão pouco a utilizem ainda e, particularmente no Brasil, exista certa falta de tradição nesse ramo da linguística aplicada.

\par Virtualmente todas as ferramentas digitais de tradução, análise linguística e construção de dicionários utilizam a linguística de corpus como base. Grandes empresas, como Google e IBM, utilizam de seu poder computacional para, com trilhões de fontes de textos disponíveis na Internet (um amontoado de corpora de domínio público esperando para ser analisado), construir conversores de texto para fala, analisadores sintáticos e tradutores automáticos.

\par No caso dos tradutores automáticos, é graças à linguística de corpus que recentemente o serviço Google Translate passou a traduzir textos em diversas línguas do e para o latim, por exemplo, o que corrobora a importância dessa área da linguística para quem estuda línguas de forma séria. Embora os resultados de ferramentas como o Google Translate não pareçam muito profissionais ainda, o fato é que traduções baseadas em estatísticas de corpora tendem a apresentar melhores resultados que as baseadas em regras linguísticas fixas \cite{och}. É por isso que muitas vezes, por exemplo, textos de um serviços como o do Google ainda demonstrem uma sintaxe estranha, mas é tudo uma questão dos corpora usados. Quanto maiores e mais abrangentes forem, melhores serão os resultados\footnote{Uma das formas mais populares para avaliação de traduções automáticas baseadas em estatísticas de corpora é a pontuação \sigla{BLEU}{Bilingual Evaluation Understudy}, que indica o grau de inteligibilidade de um texto traduzido por um computador comparando-o a um traduzido por um ser humano, atestando seu nível de qualidade.}

\chapter{Objetivos}
\par O objetivo primordial desse trabalho é permitir uma maior compreensão de textos clássicos em latim para os estudantes dessa língua através da criação de vocabulários específicos e automatizados por processamento de linguagem natural, cujas fontes serão textos de Cícero, do período clássico do latim.
 
\par Através desses resultados, formatados, alunos e professores poderão focar no aprendizado dos textos em si, não desperdiçando tempo em decorar vocabulários sem necessidade, assim como já foi feito para outras línguas, como o inglês.

\par Indiretamente, teremos um objetivo secundário que deriva do propósito anterior: a criação de corpora e um catálogo de stopwords do latim. Os corpora poderão ser utilizados para processamento de linguagem natural por terceiros, da mesma forma que serão utilizados nesse trabalho. O catálogo de stopwords poderá também ser utilizado por terceiros, e tentará ser abrangente, pois atualmente não se tem um catálogo completo para o latim assim como se tem para outras línguas.

\chapter{Metodologia}

\section{NLTK}
\par 	Nesse trabalho será utilizado o NLTK, ou Natural Language Toolkit, um conjunto de ferramentas de programação voltado ao processamento de linguagem natural através de computação. Atualmente o NLTK é um dos projetos de maior sucesso para esse fim, pois, além de ser livre de licenças e custos, é bastante abrangente e cobre diversas línguas oficialmente e é disponibilizado já com diversos corpora para testes. O projeto possui livros e publicações sobre seu funcionamento, tem mais de dez anos de desenvolvimento e é utilizado em universidades já há algum tempo, tornando-o ideal para uso pois está em constante evolução acompanhando novas descobertas no campo de processamento de linguagem natural \cite{bird}. O fato de existirem trabalhos sobre seu funcionamento, ele ter mais de uma década de existência e ser um projeto de software livre, portanto com alta expectativa de evolução e manutenção, o NLTK se mostra seguro para linguistas experimentarem-no.

\section{Corpora}
\par O escopo dos textos escolhido para compor os corpora desse trabalho abrange o período clássico da literatura latina, de qual Cícero é um dos maiores (senão o maior) expoentes. A escolha de Cícero se deu pelo fato de que sua produção foi bastante variada, tendo escrito desde tratados até cartas mais informais, de textos jurídicos até filosofia.

\par Cícero teve uma grande influência no pensamento moderno, e é ainda um dos autores mais estudados em cursos de latim, o que o torna ideal para uma análise linguística abrangente em latim. Será estudado uma das grandes características de Cícero, que é a chamada elegantia, que se traduz não como elegância simplesmente mas como um refinamento e na perfeita escolha de vocabulário para expressar uma idéia \cite{palmer}. \citeonline{kennedy} também lembra elogios do próprio Cícero para César em seu De analogia, onde este diz que a refinada escolha de palavras é uma dos vários requisitos de qualquer cidadão romano, especialmente de um grande orador como César foi na opinião de Cícero. Portanto, nada mais adequado do que tentar enxergar como isso de dava utilizando a linguística de corpus.

\par Suas coleções e períodos ainda estão sendo estudados e serão definidos em breve, mas sempre tentando construir os corpora utilizando fontes abertas ou livres de restrição, a fim de que outros possam utilizar os corpora gerados por esse trabalho sem dificuldades. Embora já existissem corpora de latim clássico (cobrindo textos e discursos de Cícero), pesquisadores e alunos somente tiram conclusões deles e não desenvolvem pesquisa sobre os dados em si: \textit{citation of this resource has now become commonplace in any article touching upon Ciceronian word usage. Perhaps because the ease of retrieval of these data makes conclusions drawn from them less prized, publication of specifically computer-based work on Ciceronian vocabulary and usage is still in the future}\cite{brill}.

\par Os corpora de Cícero serão trabalhados de acordo com as regras de formatação e organização de outros corpora encontrados no projeto NLTK; atualmente beirando quarenta corpora amplamente utilizados. Isso facilitará bastante o processamento dos textos e também contribuirá com o projeto, para aumentar ? criar, na realidade ? sua base de textos em latim. Tentaremos, se o tempo do projeto permitir, iniciar o etiquetamento gramatical (também conhecido simplesmente como tagging) dos corpora, mesmo que de forma rudimentar.

\par TODO: uma contradicao: a linguagem de cicero nao sera representativa para TODOS os autores desse periodo

\par TODO: seculo ciceroniano

\section{Stopwords}
\par O catálogo de stopwords é uma dependência natural dos corpora pois é ele que filtra resultados indesejados ou que poderiam eventualmente poluir os dados; stopwords, para um computador, nada mais são que termos que interrompem o processamento da linguagem e pula para o próximo termo. Toda língua possui uma lista de stopwords para processamento, porém o latim ainda não conta com uma verdadeiramente usável.

\par Embora pudessem ter sua criação automatizada por algorítimos de textos, catálogos de stopwords geralmente são construídos manualmente por linguistas. Logo, não existe um catálogo definitivo ou verdadeiramente completo para uma determinada língua, somente os mais abrangentes e os menos abrangentes, a depende do uso que será dado a eles.

\par Em computação, stopwords não são utilizadas somente para filtrar resultados indesejados ou que não são o foco da pesquisa linguística, muitas vezes são termos repetitivos que levam a uma pior performance de um sistema e que se não forem ignorados diminuem a legibilidade dos resultados. Por exemplo, é bastante comum filtrar-se preposições, conjunções e pronomes, uma vez que eles pouco influenciam na maioria das análises computacionais de um texto e representam um conjunto de termos funcionais ? sem muita significância ? que podem poluir os dados do ponto de vista linguístico \cite{sardinha-wordsmith} e dificultar a análise computacional \cite{makrehchi}.

\par Para o latim será criado, sob orientação, um catálogo geral de stopwords (no modelo clássico, uma lista finita de termos), e catálogos menores, onde se encontrará somente preposições, ou somente pronomes e assim por diante, para que os alunos e pesquisadores possam refinar melhor o uso de stopwords em latim.

\section{Python}
\par Tanto o NLTK quanto ferramentas auxiliares desse trabalho serão feitos utilizando a linguagem de programação Python. A escolha dessa linguagem de programação se dá, além pelo fato do NLTK utilizá-la internamente para processar línguas naturais \cite{perkins}, também pela facilidade de leitura dos códigos escritos nela por parte de pessoas que não são da área de computação e por ser uma linguagem de alto nível, ou seja, de alta abstração, o que leva a um desenvolvimento mais rápido do projeto ao mesmo tempo que permite que ele seja menos críptico para terceiros.

\section{Análise de Frequência}
\par Será usado o método de análise de frequência estatística como forma básica de obter os resultados de vocabulários dos corpora. Análise de frequência é uma forma de, através de pequenos experimentos automáticos (através de programação, tentar obter matches de um termo em um texto, por exemplo), descobrir a proporção de uso de uma palavra qualquer em relação ao todo de um corpus específico. Essa é uma das atividades primordiais da linguística de corpus, muitas vezes chamada simplesmente de estatística lexical por aderentes da linguística quantitativa, e tende a focar nas resoluções das leis de Zipf para determinados corpora \cite{tesitelova}.

\par Para chegar ao ponto de usar uma análise de frequência, é preciso antes passar pela montagem dos corpora, da criação de filtros e estipular outros parâmetros do processamento. Porém, uma vez que isso é feito, a análise de frequência se mostra relativamente simples e bastante proveitosa. Embora ela seja base da linguística quantitativa, não é preciso entrar em muitos detalhes dessa subárea da linguística.

\chapter{Cronograma}
\par As tarefas necessárias para a conclusão do trabalho devem ser divididas em etapas sequenciais, visto que uma depende da outra. Muito provavelmente as etapas serão repassadas após uma primeira "rodada" para afinar os resultados e aparar problemas encontrados durante o caminho, antes da conclusão total da monografia.

\par 	As etapas e seus tempos de conclusão serão divididas como a seguir:

\section{Definição}
\par Definir o escopo do projeto e o que se pretende fazer. Especificar limites para não perder o foco dos objetivos nem se perder em superficialidade. Este projeto de monografia propriamente dito. Tempo estimado: Set-Out 2011. Complexidade: média.

\section{Coleta}
\par Agregação do material para montagem dos corpora, pensando já na relevância dos textos, utilidade deles e abrangência linguística. Será preciso orientação próxima para que o material coletado seja relevante e representativo. Tempo estimado: Out-Nov 2011. Complexidade: média.

\section{Montagem}
\par A construção efetiva dos corpora de acordo com os formatos esperados pelo projeto NLTK e uma categorização básica dos textos para facilitar análise futura. Nessa etapa serão disponibilizados os corpora do trabalho para que outros do setor e departamento também possam utilizá-lo. Tempo estimado: Nov-Dez 2011. Complexidade: baixa.

\section{Filtragem}
\par Especificação de certos filtros a serem utilizados em análises dos corpora (como a necessidade de se catalogar stopwords da língua, por exemplo). Essa etapa também precisará de orientação próxima, pois um conhecimento profundo do latim será necessário para que os dados não fiquem poluídos. Tempo estimado: Mar 2012. Complexidade: baixa.

\section{Análise}
\par Etapa final da análise de todo o material organizado, bem como obtenção de dados estatísticos da língua para uso futuro em sala de aula ou pesquisa acadêmica. Gerar um modelo de uso do material resultante desse trabalho. Tempo estimado: Abr-Jun 2012. Complexidade: alta.

% Existem ainda: abbrv, acm, alpha, amsalpha, amsplain
\bibliographystyle{abnt-alf}	
\bibliography{monografia}

%\apendice
%\chapter{Primeiro apêndice}
%\par Apêndices são textos elaborados pelo autor a fim de complementar sua argumentação.

%\anexo
%\chapter{Primeiro anexo}
%\par Anexos são documentos não elaborados pelo autor, que servem de fundamentação, comprovação ou ilustração.

\end{document}
